{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS345_Assignment_05_Trevor_Holland.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1R4TIXGkp6l"
      },
      "source": [
        "# Introduction\n",
        "I will be using the Reuters Corpus Volume 1 dataset from scikit-learn. I am leaning heavily on the documentation from the [scikit-learn.org](https://scikit-learn.org), as well as previous lecture notebooks from this course. This is a categorization problem, taking data from news articles and determining the type of news article from the features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijU3N5860OMQ"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "import keras\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxSnvy2YvjgH"
      },
      "source": [
        "## Load the dataset\n",
        "I'm using the [Reuters Corpus Volume 1](https://scikit-learn.org/stable/datasets/real_world.html?highlight=corpus%20volume#id6) dataset from sklearn. It can often take a little while to load because it's over 800,000 entries. The target part of the dataset is similar to one hot encoding, except that the articles can be between 1 and 17 of 103 different types of articles. The target, therefore, is either easier or more difficult, depending on your point of view. Both the data and target portion of the dataset are compressed into sparse matrix format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kD11mLVCkm0m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb60f9f1-b94b-40d9-fee5-9151d89b1d9a"
      },
      "source": [
        "from sklearn.datasets import fetch_rcv1\n",
        "rcv1 = fetch_rcv1()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://ndownloader.figshare.com/files/5976069\n",
            "Downloading https://ndownloader.figshare.com/files/5976066\n",
            "Downloading https://ndownloader.figshare.com/files/5976063\n",
            "Downloading https://ndownloader.figshare.com/files/5976060\n",
            "Downloading https://ndownloader.figshare.com/files/5976057\n",
            "Downloading https://ndownloader.figshare.com/files/5976048\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytWq5GHwi10_"
      },
      "source": [
        "Both the 'data' and 'target' components of rcv1 are in a sparse matrix format. Because of the limitations of the machine, I can't access the entire data block. If you want to live dangerously, you can switch the commented lines that define X_raw and y_raw. I'd love to see what happens and how it will impact the classifiers.\n",
        "\n",
        "The target names are codes "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvS8_AKHxwxp"
      },
      "source": [
        "cutStart = 41000\n",
        "cutEnd = 47000\n",
        "X_raw = rcv1.data[cutStart:cutEnd, 40000:47000].todense() # Use this line if you are running on a mortal machine\n",
        "y_raw = rcv1.target[cutStart:cutEnd].todense() # Use this line if you are running on a mortal machine\n",
        "#X_raw = rcv1.data.todense() # Use this line only if you are confident in your amount of RAM\n",
        "#y_raw = rcv1.target.todense() # Use this line only if you used the line directly above. Live dangerously.\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhMUtCRWet2i"
      },
      "source": [
        "Below is the dictionary for converting the target name codes into English names that we can understand. I found it for RCV1 on [github](https://gist.github.com/gavinmh/6253739)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3O-SwvpYMjY"
      },
      "source": [
        "# I found this translation on GITHUB.\n",
        "code_names = { \"CCAT\": \"CORPORATE/INDUSTRIAL\", \"C11\": \"STRATEGY/PLANS\", \"C12\": \"LEGAL/JUDICIAL\", \"C13\": \"REGULATION/POLICY\",\n",
        "              \"C14\": \"SHARE LISTINGS\", \"C15\": \"PERFORMANCE\", \"C151\": \"ACCOUNTS/EARNINGS\", \"C1511\": \"ANNUAL RESULTS\", \"C152\": \"COMMENT/FORECASTS\",\n",
        "              \"C16\": \"INSOLVENCY/LIQUIDITY\", \"C17\": \"FUNDING/CAPITAL\", \"C171\": \"SHARE CAPITAL\", \"C172\": \"BONDS/DEBT ISSUES\",\n",
        "              \"C173\": \"LOANS/CREDITS\", \"C174\": \"CREDIT RATINGS\", \"C18\": \"OWNERSHIP CHANGES\", \"C181\": \"MERGERS/ACQUISITIONS\",\n",
        "              \"C182\": \"ASSET TRANSFERS\", \"C183\": \"PRIVATISATIONS\", \"C21\": \"PRODUCTION/SERVICES\", \"C22\": \"NEW PRODUCTS/SERVICES\",\n",
        "              \"C23\": \"RESEARCH/DEVELOPMENT\", \"C24\": \"CAPACITY/FACILITIES\", \"C31\": \"MARKETS/MARKETING\", \"C311\": \"DOMESTIC MARKETS\",\n",
        "              \"C312\": \"EXTERNAL MARKETS\", \"C313\": \"MARKET SHARE\", \"C32\": \"ADVERTISING/PROMOTION\", \"C33\": \"CONTRACTS/ORDERS\",\n",
        "              \"C331\": \"DEFENCE CONTRACTS\", \"C34\": \"MONOPOLIES/COMPETITION\", \"C41\": \"MANAGEMENT\", \"C411\": \"MANAGEMENT MOVES\",\n",
        "              \"C42\": \"LABOUR\", \"CCAT\": \"ECONOMICS\", \"E11\": \"ECONOMIC PERFORMANCE\", \"E12\": \"MONETARY/ECONOMIC\", \"E121\": \"MONEY SUPPLY\",\n",
        "              \"E13\": \"INFLATION/PRICES\", \"E131\": \"CONSUMER PRICES\", \"E132\": \"WHOLESALE PRICES\", \"E14\": \"CONSUMER FINANCE\", \"E141\": \"PERSONAL INCOME\",\n",
        "              \"E142\": \"CONSUMER CREDIT\", \"E143\": \"RETAIL SALES\", \"E21\": \"GOVERNMENT FINANCE\", \"E211\": \"EXPENDITURE/REVENUE\",\n",
        "              \"E212\": \"GOVERNMENT BORROWING\", \"E31\": \"OUTPUT/CAPACITY\", \"E311\": \"INDUSTRIAL PRODUCTION\", \"E312\": \"CAPACITY UTILIZATION\",\n",
        "              \"E313\": \"INVENTORIES\", \"E41\": \"EMPLOYMENT/LABOUR\", \"E411\": \"UNEMPLOYMENT\", \"E51\": \"TRADE/RESERVES\", \"E511\": \"BALANCE OF PAYMENTS\",\n",
        "              \"E512\": \"MERCHANDISE TRADE\", \"E513\": \"RESERVES\", \"E61\": \"HOUSING STARTS\", \"E71\": \"LEADING INDICATORS\", \"GCAT\": \"GOVERNMENT/SOCIAL\",\n",
        "              \"G15\": \"EUROPEAN COMMUNITY\", \"G151\": \"EC INTERNAL MARKET\", \"G152\": \"EC CORPORATE POLICY\", \"G153\": \"EC AGRICULTURE POLICY\", \n",
        "              \"G154\": \"EC MONETARY/ECONOMIC\", \"G155\": \"EC INSTITUTIONS\", \"G156\": \"EC ENVIRONMENT ISSUES\", \"G157\": \"EC COMPETITION/SUBSIDY\",\n",
        "              \"G158\": \"EC EXTERNAL RELATIONS\", \"G159\": \"EC GENERAL\", \"GCRIM\": \"CRIME, LAW ENFORCEMENT\", \"GDEF\": \"DEFENCE\", \n",
        "              \"GDIP\": \"INTERNATIONAL RELATIONS\", \"GDIS\": \"DISASTERS AND ACCIDENTS\", \"GENT\": \"ARTS, CULTURE, ENTERTAINMENT\",\n",
        "              \"GENV\": \"ENVIRONMENT AND NATURAL WORLD\", \"GFAS\": \"FASHION\", \"GHEA\": \"HEALTH\", \"GJOB\": \"LABOUR ISSUES\", \"GMIL\": \"MILLENNIUM ISSUES\",\n",
        "              \"GOBIT\": \"OBITUARIES\", \"GODD\": \"HUMAN INTEREST\", \"GPOL\": \"DOMESTIC POLITICS\", \"GPRO\": \"BIOGRAPHIES, PERSONALITIES, PEOPLE\",\n",
        "              \"GREL\": \"RELIGION\", \"GSCI\": \"SCIENCE AND TECHNOLOGY\", \"GSPO\": \"SPORTS\", \"GTOUR\": \"TRAVEL AND TOURISM\", \"GVIO\": \"WAR, CIVIL WAR\",\n",
        "              \"GVOTE\": \"ELECTIONS\", \"GWEA\": \"WEATHER\", \"GWELF\": \"WELFARE, SOCIAL SERVICES\", \"MCAT\": \"MARKETS\", \"M11\": \"EQUITY MARKETS\",\n",
        "              \"M12\": \"BOND MARKETS\", \"M13\": \"MONEY MARKETS\", \"M131\": \"INTERBANK MARKETS\", \"M132\": \"FOREX MARKETS\", \"M14\": \"COMMODITY MARKETS\",\n",
        "              \"M141\": \"SOFT COMMODITIES\", \"M142\": \"METALS TRADING\", \"M143\": \"ENERGY MARKETS\", \"ECAT\": \"ECONOMIC/SOCIAL\"}\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPV8BDsRvvSS"
      },
      "source": [
        "# Set up Models\n",
        "I will be using a Decision Tree and a Neural Net model. I'm using an 80/20 split for training and test data. I'm using a variation of the function used in Assignment 04."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXu1gfGXWt4t"
      },
      "source": [
        "# Function to split data, inspired by Assignment 04\n",
        "data_split_ratio = 0.8\n",
        "\n",
        "def get_train_test(data, targets, ratio):\n",
        "  mask = np.random.rand(len(data)) < ratio\n",
        "  data_train = data[mask]\n",
        "  data_test = data[~mask]\n",
        "  target_train = targets[mask]\n",
        "  target_test = targets[~mask]\n",
        "  return data_train, target_train, data_test, target_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oips88z8nNww"
      },
      "source": [
        "X_train, y_train, X_test, y_test = get_train_test(X_raw, y_raw, data_split_ratio)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRVXD_caVD-V"
      },
      "source": [
        "### Decision Tree\n",
        "A single line sets up a decision tree. At a depth of 70, that's about the best that the decision tree will get with both training data and test data. It is probably over-fitting the training data, but there are marginal gains (it increases in accuracy slowly from the low 0.20s to the high 0.20s) between a depth of 30 and 70 for the test data. The training data caps out at an accuracy of roughly 0.98."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZpvoW2QwaUr"
      },
      "source": [
        "# Setting the decision tree\n",
        "tree_depth = 70\n",
        "tree = DecisionTreeClassifier(max_depth=tree_depth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ehesrwXVKlD"
      },
      "source": [
        "### Neural Net\n",
        "Only using a single layer because otherwise everything crashes. The final accuracy will be impacted by this limitation.\n",
        "\n",
        "I played with different optimizers before settling on the gradient descent (`SGD()`) optimizer. The `.CategoricalCrossentropy()` method seems to be the best way to display loss at each epoch, but MSE is present in the commented section as well for comparison. I wanted to make it three or more internal layers deep, but I run out of RAM on Colab when I try to do even two layers. I'd be interested to see what happens when you can run the full dataset through a multi-layer neural net. I guess that it stands a good chance of improving the accuracy of the model, but it would definitely bog down the performance of the machine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Xm2E_mVVOpz"
      },
      "source": [
        "full_size = X_raw.shape[1]\n",
        "activators = y_raw.shape[1]\n",
        "network_nn =  keras.Sequential(name=\"Neural_Network\")\n",
        "#network_nn.add(keras.layers.Flatten()) # I'm not using images, so I don't think I need this.\n",
        "network_nn.add(keras.layers.Dense(full_size, activation='relu'))\n",
        "network_nn.add(keras.layers.Dense(full_size, activation='relu')) # My hardware can't handle this\n",
        "#network_nn.add(keras.layers.Dense(full_size, activation='relu')) # My hardware can't handle this\n",
        "network_nn.add(keras.layers.Dense(activators, activation='softmax'))\n",
        "#loss_fn = keras.losses.MeanSquaredError(reduction=\"auto\", name=\"mean_squared_error\")\n",
        "loss_fn = keras.losses.CategoricalCrossentropy()\n",
        "opt = keras.optimizers.SGD()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9pT7k6twa4q"
      },
      "source": [
        "## Run the models\n",
        "The decision tree model is first because it takes less processing, less RAM, and less time. The Neural Net is second because it takes more of all of those things."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kA2agri6lJ5n"
      },
      "source": [
        "### Decision Tree model\n",
        "Very simple: fit the data to the tree. Call the `.fit()` method and let it run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJT7T1hswgIF"
      },
      "source": [
        "# Fitting the data\n",
        "tree.fit(X_train, y_train);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u91Yfi8Cmlz4"
      },
      "source": [
        "### Decision Tree Performance\n",
        "The performance of the tree is accessable through the .score() method of DecisionTreeClassifier. Handy!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eQEjU2twk_I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "739be33b-dd31-4137-8599-6e36ce97a888"
      },
      "source": [
        "print(f\"Training score: {tree.score(X_train,y_train):0.5f}\")\n",
        "print(f\"Testing score : {tree.score(X_test,y_test):0.5f}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training score: 0.99367\n",
            "Testing score : 0.14286\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMhqqwT-pxbe"
      },
      "source": [
        "### Nerual Network model\n",
        "There are a lot of limitations going on with the neural net here. The fact that I can't use the entirety of the feature set probably plays a part in it, although having 47k+ features is a huge amount of features. One of two things must be happening: either neural nets are not very good for this type of problem or I don't have the necessary skill to set up a neural net to solve this particular problem. There are enough confounding variables for me that I'm not sure which it is.\n",
        "\n",
        "I found the print of the loss and accuracy from Module 9 useful, so I used that function here. I also read the github article from which it came, but I don't think I was having that issue with the Neural Net. I did try the net without it, just to see, and I didn't think the output was as useful."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ap6X4ku1txEa"
      },
      "source": [
        "# Structure based on https://github.com/keras-team/keras/issues/2548\n",
        "# Taken from module 9\n",
        "class EvaluateCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, test_data):\n",
        "        self.test_data = test_data\n",
        "        \n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        x, y = self.test_data\n",
        "        loss, acc = self.model.evaluate(x, y, verbose=0)\n",
        "        if 'test loss' not in logs:\n",
        "            logs['test loss'] = []\n",
        "            logs['test acc'] = []\n",
        "        logs['test loss'] += [loss]\n",
        "        logs['test acc'] += [acc]\n",
        "        print('Testing loss: {}, acc: {}\\n'.format(round(loss, 4), round(acc, 4)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDjX81vHKY3J"
      },
      "source": [
        "The neural network accuracy depends on a lot of factors, but I have yet to see a run hit 0.1. Changing the batch size can help, to a point, but mostly it makes training and evaluating take longer. The number of epochs past 10 doesn't help the accuracy either, but it's easy enough to change it to see."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJc_WQFBp0Iv",
        "outputId": "7ff81ae9-fb76-423a-f04c-9202da1c1c27"
      },
      "source": [
        "network_nn_epochs = 10\n",
        "network_nn.compile(loss=loss_fn, optimizer=opt, metrics=['accuracy'])\n",
        "history = network_nn.fit(X_train, y_train, batch_size=50, epochs=network_nn_epochs, verbose=1,\n",
        "                         callbacks=[EvaluateCallback((X_test, y_test))])\n",
        "network_nn.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            " 4/97 [>.............................] - ETA: 1s - loss: 0.0330 - accuracy: 0.0054     WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0077s vs `on_train_batch_end` time: 0.0119s). Check your callbacks.\n",
            "97/97 [==============================] - 2s 16ms/step - loss: 0.0316 - accuracy: 0.0061\n",
            "Testing loss: 0.0314, acc: 0.0078\n",
            "\n",
            "Epoch 2/15\n",
            "97/97 [==============================] - 1s 15ms/step - loss: 0.0314 - accuracy: 0.0057\n",
            "Testing loss: 0.0314, acc: 0.0078\n",
            "\n",
            "Epoch 3/15\n",
            "97/97 [==============================] - 1s 15ms/step - loss: 0.0312 - accuracy: 0.0073\n",
            "Testing loss: 0.0314, acc: 0.0078\n",
            "\n",
            "Epoch 4/15\n",
            "97/97 [==============================] - 1s 15ms/step - loss: 0.0313 - accuracy: 0.0058\n",
            "Testing loss: 0.0314, acc: 0.0078\n",
            "\n",
            "Epoch 5/15\n",
            "97/97 [==============================] - 1s 15ms/step - loss: 0.0312 - accuracy: 0.0064\n",
            "Testing loss: 0.0314, acc: 0.0078\n",
            "\n",
            "Epoch 6/15\n",
            "97/97 [==============================] - 1s 15ms/step - loss: 0.0318 - accuracy: 0.0062\n",
            "Testing loss: 0.0314, acc: 0.0069\n",
            "\n",
            "Epoch 7/15\n",
            "97/97 [==============================] - 1s 15ms/step - loss: 0.0316 - accuracy: 0.0059\n",
            "Testing loss: 0.0314, acc: 0.006\n",
            "\n",
            "Epoch 8/15\n",
            "97/97 [==============================] - 1s 15ms/step - loss: 0.0315 - accuracy: 0.0077\n",
            "Testing loss: 0.0314, acc: 0.0052\n",
            "\n",
            "Epoch 9/15\n",
            "97/97 [==============================] - 1s 15ms/step - loss: 0.0312 - accuracy: 0.0092\n",
            "Testing loss: 0.0314, acc: 0.0052\n",
            "\n",
            "Epoch 10/15\n",
            "97/97 [==============================] - 1s 15ms/step - loss: 0.0315 - accuracy: 0.0096\n",
            "Testing loss: 0.0314, acc: 0.0052\n",
            "\n",
            "Epoch 11/15\n",
            "97/97 [==============================] - 1s 15ms/step - loss: 0.0319 - accuracy: 0.0065\n",
            "Testing loss: 0.0314, acc: 0.0043\n",
            "\n",
            "Epoch 12/15\n",
            "97/97 [==============================] - 1s 15ms/step - loss: 0.0318 - accuracy: 0.0071\n",
            "Testing loss: 0.0314, acc: 0.0052\n",
            "\n",
            "Epoch 13/15\n",
            "97/97 [==============================] - 1s 15ms/step - loss: 0.0316 - accuracy: 0.0104\n",
            "Testing loss: 0.0314, acc: 0.0052\n",
            "\n",
            "Epoch 14/15\n",
            "97/97 [==============================] - 1s 15ms/step - loss: 0.0316 - accuracy: 0.0095\n",
            "Testing loss: 0.0314, acc: 0.0043\n",
            "\n",
            "Epoch 15/15\n",
            "97/97 [==============================] - 1s 15ms/step - loss: 0.0314 - accuracy: 0.0092\n",
            "Testing loss: 0.0314, acc: 0.006\n",
            "\n",
            "Model: \"Neural_Network\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_14 (Dense)             (None, 7000)              49007000  \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 7000)              49007000  \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 103)               721103    \n",
            "=================================================================\n",
            "Total params: 98,735,103\n",
            "Trainable params: 98,735,103\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiaYQm66woLX"
      },
      "source": [
        "# Conclusions and Observations\n",
        "First: don't use a giant dataset without serious computing power. Paring everything down to operate on the limited RAM and GPU resources means I don't know much about what features are most important and the shear size of the feature list means that it would be impractical for me to read all of it.\n",
        "\n",
        "Second: it's not difficult to set up a problem for an algorithm. The APIs are easy to use. Finding an algorithm that will work best for your dataset is a much more difficult problem.\n",
        "\n",
        "Third: ambition can really make things more difficult than they have to be. I wanted to see what happens when you have a big dataset and I found out that the problem might be intractible with hardware limitations.\n",
        "\n",
        "Most importantly: I chose a large dataset and a classification problem (although not an image classification problem). It's similar in many ways to the iris or mushroom classifier problems, but with a lot less clarity. I knew this going in and thought that I could just set up the problem and let time and computing solve it. That was hubris on my part, but also not yet understanding the exact limitations of the software and the hardware.\n",
        "\n",
        "The Decision Tree has a much greater accuracy than chance while the Neural Net is roughly chance. I think that a huge part of that is that I can't set up the Neural Net I want to set up due to processing constraints. The Decision Tree does much better than chance and I think that's because the Decision Tree works very well on finding a feature to make an A/B choice. It isn't a perfect classifier though, and I think for this dataset it would perform better with access to more of the features so that it could figure out more ways to classify. The other issue I think that comes into play is that there are some elements of the dataset that qualify as multiple types of article. It would be like classifying a tiger and a bobcat and a housecat: they all should be cats, but some of them are also big cats or wild cats.\n",
        "\n",
        "You can see in the segment below a list of all of the article types and that there are several that overlap."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SUweFs5fWMX",
        "outputId": "35407c92-6800-4e2d-8d9e-2c8791d9551e"
      },
      "source": [
        "for e in rcv1.target_names:\n",
        "  print(code_names[e])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "STRATEGY/PLANS\n",
            "LEGAL/JUDICIAL\n",
            "REGULATION/POLICY\n",
            "SHARE LISTINGS\n",
            "PERFORMANCE\n",
            "ACCOUNTS/EARNINGS\n",
            "ANNUAL RESULTS\n",
            "COMMENT/FORECASTS\n",
            "INSOLVENCY/LIQUIDITY\n",
            "FUNDING/CAPITAL\n",
            "SHARE CAPITAL\n",
            "BONDS/DEBT ISSUES\n",
            "LOANS/CREDITS\n",
            "CREDIT RATINGS\n",
            "OWNERSHIP CHANGES\n",
            "MERGERS/ACQUISITIONS\n",
            "ASSET TRANSFERS\n",
            "PRIVATISATIONS\n",
            "PRODUCTION/SERVICES\n",
            "NEW PRODUCTS/SERVICES\n",
            "RESEARCH/DEVELOPMENT\n",
            "CAPACITY/FACILITIES\n",
            "MARKETS/MARKETING\n",
            "DOMESTIC MARKETS\n",
            "EXTERNAL MARKETS\n",
            "MARKET SHARE\n",
            "ADVERTISING/PROMOTION\n",
            "CONTRACTS/ORDERS\n",
            "DEFENCE CONTRACTS\n",
            "MONOPOLIES/COMPETITION\n",
            "MANAGEMENT\n",
            "MANAGEMENT MOVES\n",
            "LABOUR\n",
            "ECONOMICS\n",
            "ECONOMIC PERFORMANCE\n",
            "MONETARY/ECONOMIC\n",
            "MONEY SUPPLY\n",
            "INFLATION/PRICES\n",
            "CONSUMER PRICES\n",
            "WHOLESALE PRICES\n",
            "CONSUMER FINANCE\n",
            "PERSONAL INCOME\n",
            "CONSUMER CREDIT\n",
            "RETAIL SALES\n",
            "GOVERNMENT FINANCE\n",
            "EXPENDITURE/REVENUE\n",
            "GOVERNMENT BORROWING\n",
            "OUTPUT/CAPACITY\n",
            "INDUSTRIAL PRODUCTION\n",
            "CAPACITY UTILIZATION\n",
            "INVENTORIES\n",
            "EMPLOYMENT/LABOUR\n",
            "UNEMPLOYMENT\n",
            "TRADE/RESERVES\n",
            "BALANCE OF PAYMENTS\n",
            "MERCHANDISE TRADE\n",
            "RESERVES\n",
            "HOUSING STARTS\n",
            "LEADING INDICATORS\n",
            "ECONOMIC/SOCIAL\n",
            "EUROPEAN COMMUNITY\n",
            "EC INTERNAL MARKET\n",
            "EC CORPORATE POLICY\n",
            "EC AGRICULTURE POLICY\n",
            "EC MONETARY/ECONOMIC\n",
            "EC INSTITUTIONS\n",
            "EC ENVIRONMENT ISSUES\n",
            "EC COMPETITION/SUBSIDY\n",
            "EC EXTERNAL RELATIONS\n",
            "EC GENERAL\n",
            "GOVERNMENT/SOCIAL\n",
            "CRIME, LAW ENFORCEMENT\n",
            "DEFENCE\n",
            "INTERNATIONAL RELATIONS\n",
            "DISASTERS AND ACCIDENTS\n",
            "ARTS, CULTURE, ENTERTAINMENT\n",
            "ENVIRONMENT AND NATURAL WORLD\n",
            "FASHION\n",
            "HEALTH\n",
            "LABOUR ISSUES\n",
            "MILLENNIUM ISSUES\n",
            "OBITUARIES\n",
            "HUMAN INTEREST\n",
            "DOMESTIC POLITICS\n",
            "BIOGRAPHIES, PERSONALITIES, PEOPLE\n",
            "RELIGION\n",
            "SCIENCE AND TECHNOLOGY\n",
            "SPORTS\n",
            "TRAVEL AND TOURISM\n",
            "WAR, CIVIL WAR\n",
            "ELECTIONS\n",
            "WEATHER\n",
            "WELFARE, SOCIAL SERVICES\n",
            "EQUITY MARKETS\n",
            "BOND MARKETS\n",
            "MONEY MARKETS\n",
            "INTERBANK MARKETS\n",
            "FOREX MARKETS\n",
            "COMMODITY MARKETS\n",
            "SOFT COMMODITIES\n",
            "METALS TRADING\n",
            "ENERGY MARKETS\n",
            "MARKETS\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}